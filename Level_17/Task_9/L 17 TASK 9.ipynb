{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d47024-43fa-4e63-8cc0-00732b5927b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccade44c-35b3-4093-abee-a9e079722d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 1, 1, 1),\n",
    "    (1, 1, 2, 0),\n",
    "    (1, 1, 3, 4),\n",
    "    (1, 1, 4, 2),\n",
    "    (1, 2, 1, 6),\n",
    "    (1, 2, 2, 1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d71f816-bcda-466b-a291-83752f38706a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = [\"match_id\", \"inning\", \"ball_no\", \"runs\"]\n",
    "df = spark.createDataFrame(data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a87ff8-11d5-4ebf-b552-0f1a50b9a3d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"match_id\", \"inning\").orderBy(\"ball_no\")\n",
    "df_with_cumulative_runs = df.withColumn(\n",
    "    \"cumulative_runs\", F.sum(\"runs\").over(windowSpec)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aca8a81-4468-4088-8a39-d3a0f3f78532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+----+---------------+\n|match_id|inning|ball_no|runs|cumulative_runs|\n+--------+------+-------+----+---------------+\n|1       |1     |1      |1   |1              |\n|1       |1     |2      |0   |1              |\n|1       |1     |3      |4   |5              |\n|1       |1     |4      |2   |7              |\n|1       |2     |1      |6   |6              |\n|1       |2     |2      |1   |7              |\n+--------+------+-------+----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_with_cumulative_runs.select(\"match_id\", \"inning\", \"ball_no\", \"runs\", \"cumulative_runs\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b1ca63-8b98-4cd0-b1d2-8e3dd4db61d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+------+-------+------+--------+------+-----------+\n|commentary                                          |bowler|batsman|runs  |match_id|inning|ball_no    |\n+----------------------------------------------------+------+-------+------+--------+------+-----------+\n|Bumrah to Dhawan, FOUR! Cracking shot through covers|Bumrah|Dhawan |FOUR  |1       |1     |8589934592 |\n|Chahal to Raina, 1 run, nudged to midwicket         |Chahal|Raina  |1 run |1       |1     |25769803776|\n|Narine to Kohli, no run, defended solidly           |Narine|Kohli  |no run|1       |1     |42949672960|\n|Rabada to Rohit, SIX! Smashed over long-on          |Rabada|Rohit  |SIX   |1       |1     |60129542144|\n+----------------------------------------------------+------+-------+------+--------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "data = [\n",
    "    (\"Bumrah to Dhawan, FOUR! Cracking shot through covers\", \"Bumrah\", \"Dhawan\", \"FOUR\"),\n",
    "    (\"Chahal to Raina, 1 run, nudged to midwicket\", \"Chahal\", \"Raina\", \"1 run\"),\n",
    "    (\"Narine to Kohli, no run, defended solidly\", \"Narine\", \"Kohli\", \"no run\"),\n",
    "    (\"Rabada to Rohit, SIX! Smashed over long-on\", \"Rabada\", \"Rohit\", \"SIX\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"commentary\", StringType(), True),\n",
    "    StructField(\"bowler\", StringType(), True),\n",
    "    StructField(\"batsman\", StringType(), True),\n",
    "    StructField(\"runs\", StringType(), True)\n",
    "])\n",
    "\n",
    "commentary_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "commentary_df = commentary_df.withColumn(\"match_id\", lit(1)) \\\n",
    "                             .withColumn(\"inning\", lit(1)) \\\n",
    "                             .withColumn(\"ball_no\", monotonically_increasing_id())\n",
    "\n",
    "commentary_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271e6025-e895-4e01-b532-45e2b26976f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-------+----+---------------+\n|match_id|inning|ball_no    |batsman|runs|cumulative_runs|\n+--------+------+-----------+-------+----+---------------+\n|1       |1     |8589934592 |Dhawan |4   |4              |\n|1       |1     |25769803776|Raina  |1   |5              |\n|1       |1     |42949672960|Kohli  |0   |5              |\n|1       |1     |60129542144|Rohit  |6   |11             |\n+--------+------+-----------+-------+----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy(\"match_id\", \"inning\").orderBy(\"ball_no\")\n",
    "df_with_cumulative_runs = df_extracted.withColumn(\n",
    "    \"cumulative_runs\", \n",
    "    F.sum(\"runs\").over(window_spec)\n",
    ")\n",
    "df_with_cumulative_runs.select(\"match_id\", \"inning\", \"ball_no\", \"batsman\", \"runs\", \"cumulative_runs\").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "L 17 TASK 9",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}